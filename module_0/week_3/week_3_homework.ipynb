{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv.zip\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "Id                                                               \n",
       "1        2596      51      3                               258   \n",
       "2        2590      56      2                               212   \n",
       "3        2804     139      9                               268   \n",
       "4        2785     155     18                               242   \n",
       "5        2595      45      2                               153   \n",
       "\n",
       "    Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "Id                                                                    \n",
       "1                                0                              510   \n",
       "2                               -6                              390   \n",
       "3                               65                             3180   \n",
       "4                              118                             3090   \n",
       "5                               -1                              391   \n",
       "\n",
       "    Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "Id                                                 \n",
       "1             221             232            148   \n",
       "2             220             235            151   \n",
       "3             234             238            135   \n",
       "4             238             238            122   \n",
       "5             220             234            150   \n",
       "\n",
       "    Horizontal_Distance_To_Fire_Points     ...      Soil_Type32  Soil_Type33  \\\n",
       "Id                                         ...                                 \n",
       "1                                 6279     ...                0            0   \n",
       "2                                 6225     ...                0            0   \n",
       "3                                 6121     ...                0            0   \n",
       "4                                 6211     ...                0            0   \n",
       "5                                 6172     ...                0            0   \n",
       "\n",
       "    Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  Soil_Type38  \\\n",
       "Id                                                                    \n",
       "1             0            0            0            0            0   \n",
       "2             0            0            0            0            0   \n",
       "3             0            0            0            0            0   \n",
       "4             0            0            0            0            0   \n",
       "5             0            0            0            0            0   \n",
       "\n",
       "    Soil_Type39  Soil_Type40  Cover_Type  \n",
       "Id                                        \n",
       "1             0            0           5  \n",
       "2             0            0           5  \n",
       "3             0            0           2  \n",
       "4             0            0           2  \n",
       "5             0            0           5  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forest Cover Dataset\n",
    "\n",
    "This is the 'sort of famous' forest cover dataset.  Our goal is to predict the forest cover type, given the other variables.\n",
    "\n",
    "That means that'Cover_Type' is our dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1:  What columns are present in the dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: What is the distribution of the dependent variable?\n",
    "\n",
    "It has a mean of 4 with a STD of 2, max is 7 min is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Cover_Type'].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv(\"train.csv.zip\", index_col=0)\n",
    "    y = df.Cover_Type\n",
    "    y = to_categorical(y)\n",
    "    X = df.drop([\"Cover_Type\"], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: In the above code, what purpose does to_categorical serve?\n",
    "In the above, the to_categorical serves the purpose much like it says to make the variable a category. The overall purpose would be that when the network performs its task, instead of the ouput being numerical per se, which can have any real number value, it forces the output to be one of a finite number of categories. You would not want an output of 2.342938598 to represent a Cover_Type of 2. Thus it would force the output to be a 2 or 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(input_features=None):\n",
    "    inputs = Input(shape=(input_features,), name=\"input\")\n",
    "    #implement the code to create a keras MLP here.  \n",
    "    # remember your output layer will need to use softmax activation\n",
    "    x=Dense(54, activation='relu', name=\"hidden1\")(inputs)\n",
    "    #x=Dropout(0.5)(x)\n",
    "    x=Dense(54, activation='relu', name=\"hidden2\")(x)\n",
    "    #x=Dropout(0.5)(x)\n",
    "    x=Dense(54, activation='relu', name=\"hidden3\")(x)\n",
    "    #x=Dropout(0.5)(x)\n",
    "    x=Dense(54, activation='relu', name=\"hidden4\")(x)\n",
    "    #x=Dropout(0.5)(x)\n",
    "    #x=Dense(26, activation='relu', name=\"hidden5\")(x)\n",
    "    #x=Dropout(0.5)(x)\n",
    "   \n",
    "    prediction = Dense(8, activation='softmax', name=\"output\")(x)\n",
    "    model = Model(inputs=inputs, outputs=prediction)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks():\n",
    "    fn = os.getcwd()\n",
    "    tensorboard_callback=TensorBoard(log_dir=fn+'\\\\logs',\n",
    "                                    histogram_freq=1, batch_size=32, write_graph=True,\n",
    "                                    write_grads=False)\n",
    "    return [tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler = load_data()\n",
    "callbacks = create_callbacks()\n",
    "model = build_network(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10584 samples, validate on 2268 samples\n",
      "Epoch 1/50\n",
      "10584/10584 [==============================] - 1s 51us/step - loss: 1.3935 - acc: 0.4859 - val_loss: 0.8713 - val_acc: 0.6332\n",
      "Epoch 2/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.8147 - acc: 0.6597 - val_loss: 0.7669 - val_acc: 0.6693\n",
      "Epoch 3/50\n",
      "10584/10584 [==============================] - 0s 21us/step - loss: 0.7216 - acc: 0.6994 - val_loss: 0.7054 - val_acc: 0.7006\n",
      "Epoch 4/50\n",
      "10584/10584 [==============================] - 0s 21us/step - loss: 0.6646 - acc: 0.7190 - val_loss: 0.6763 - val_acc: 0.7006\n",
      "Epoch 5/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.6311 - acc: 0.7387 - val_loss: 0.6441 - val_acc: 0.7244\n",
      "Epoch 6/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.6072 - acc: 0.7448 - val_loss: 0.6448 - val_acc: 0.7275\n",
      "Epoch 7/50\n",
      "10584/10584 [==============================] - 0s 21us/step - loss: 0.5889 - acc: 0.7542 - val_loss: 0.6181 - val_acc: 0.7403\n",
      "Epoch 8/50\n",
      "10584/10584 [==============================] - 0s 22us/step - loss: 0.5688 - acc: 0.7617 - val_loss: 0.6048 - val_acc: 0.7491\n",
      "Epoch 9/50\n",
      "10584/10584 [==============================] - 0s 23us/step - loss: 0.5553 - acc: 0.7705 - val_loss: 0.5923 - val_acc: 0.7496\n",
      "Epoch 10/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.5482 - acc: 0.7713 - val_loss: 0.5787 - val_acc: 0.7575\n",
      "Epoch 11/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.5386 - acc: 0.7759 - val_loss: 0.5835 - val_acc: 0.7628\n",
      "Epoch 12/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.5310 - acc: 0.7785 - val_loss: 0.5689 - val_acc: 0.7575\n",
      "Epoch 13/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.5176 - acc: 0.7831 - val_loss: 0.5551 - val_acc: 0.7694\n",
      "Epoch 14/50\n",
      "10584/10584 [==============================] - 0s 19us/step - loss: 0.5045 - acc: 0.7909 - val_loss: 0.5758 - val_acc: 0.7579\n",
      "Epoch 15/50\n",
      "10584/10584 [==============================] - 0s 19us/step - loss: 0.5036 - acc: 0.7899 - val_loss: 0.5618 - val_acc: 0.7632\n",
      "Epoch 16/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.4911 - acc: 0.7929 - val_loss: 0.5529 - val_acc: 0.7707\n",
      "Epoch 17/50\n",
      "10584/10584 [==============================] - 0s 19us/step - loss: 0.4850 - acc: 0.7977 - val_loss: 0.5434 - val_acc: 0.7716\n",
      "Epoch 18/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.4789 - acc: 0.7997 - val_loss: 0.5639 - val_acc: 0.7659\n",
      "Epoch 19/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.4705 - acc: 0.8066 - val_loss: 0.5587 - val_acc: 0.7685\n",
      "Epoch 20/50\n",
      "10584/10584 [==============================] - 0s 22us/step - loss: 0.4675 - acc: 0.8050 - val_loss: 0.5430 - val_acc: 0.7725\n",
      "Epoch 21/50\n",
      "10584/10584 [==============================] - 0s 21us/step - loss: 0.4638 - acc: 0.8122 - val_loss: 0.5349 - val_acc: 0.7738\n",
      "Epoch 22/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.4554 - acc: 0.8121 - val_loss: 0.5190 - val_acc: 0.7853\n",
      "Epoch 23/50\n",
      "10584/10584 [==============================] - 0s 22us/step - loss: 0.4457 - acc: 0.8146 - val_loss: 0.5272 - val_acc: 0.7928\n",
      "Epoch 24/50\n",
      "10584/10584 [==============================] - 0s 23us/step - loss: 0.4460 - acc: 0.8163 - val_loss: 0.5654 - val_acc: 0.7747\n",
      "Epoch 25/50\n",
      "10584/10584 [==============================] - 0s 22us/step - loss: 0.4347 - acc: 0.8204 - val_loss: 0.5307 - val_acc: 0.7848\n",
      "Epoch 26/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.4369 - acc: 0.8178 - val_loss: 0.5254 - val_acc: 0.7937\n",
      "Epoch 27/50\n",
      "10584/10584 [==============================] - 0s 19us/step - loss: 0.4225 - acc: 0.8235 - val_loss: 0.5064 - val_acc: 0.7994\n",
      "Epoch 28/50\n",
      "10584/10584 [==============================] - 0s 21us/step - loss: 0.4179 - acc: 0.8293 - val_loss: 0.5177 - val_acc: 0.7862\n",
      "Epoch 29/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.4188 - acc: 0.8259 - val_loss: 0.5149 - val_acc: 0.8029\n",
      "Epoch 30/50\n",
      "10584/10584 [==============================] - 0s 22us/step - loss: 0.4122 - acc: 0.8300 - val_loss: 0.5234 - val_acc: 0.7941\n",
      "Epoch 31/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.4082 - acc: 0.8289 - val_loss: 0.5024 - val_acc: 0.8056\n",
      "Epoch 32/50\n",
      "10584/10584 [==============================] - 0s 21us/step - loss: 0.4057 - acc: 0.8311 - val_loss: 0.5647 - val_acc: 0.7791\n",
      "Epoch 33/50\n",
      "10584/10584 [==============================] - 0s 22us/step - loss: 0.4022 - acc: 0.8338 - val_loss: 0.5052 - val_acc: 0.8086\n",
      "Epoch 34/50\n",
      "10584/10584 [==============================] - 0s 22us/step - loss: 0.3958 - acc: 0.8366 - val_loss: 0.4944 - val_acc: 0.8100\n",
      "Epoch 35/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.3865 - acc: 0.8399 - val_loss: 0.4937 - val_acc: 0.7998\n",
      "Epoch 36/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.3816 - acc: 0.8433 - val_loss: 0.5103 - val_acc: 0.7985\n",
      "Epoch 37/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.3786 - acc: 0.8439 - val_loss: 0.5004 - val_acc: 0.7963\n",
      "Epoch 38/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.3720 - acc: 0.8444 - val_loss: 0.4969 - val_acc: 0.8029\n",
      "Epoch 39/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.3679 - acc: 0.8473 - val_loss: 0.5157 - val_acc: 0.8029\n",
      "Epoch 40/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.3690 - acc: 0.8498 - val_loss: 0.5236 - val_acc: 0.8016\n",
      "Epoch 41/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.3627 - acc: 0.8511 - val_loss: 0.4787 - val_acc: 0.8170\n",
      "Epoch 42/50\n",
      "10584/10584 [==============================] - 0s 21us/step - loss: 0.3538 - acc: 0.8564 - val_loss: 0.4980 - val_acc: 0.7963\n",
      "Epoch 43/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.3526 - acc: 0.8580 - val_loss: 0.4910 - val_acc: 0.8108\n",
      "Epoch 44/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.3465 - acc: 0.8605 - val_loss: 0.4955 - val_acc: 0.8179\n",
      "Epoch 45/50\n",
      "10584/10584 [==============================] - 0s 19us/step - loss: 0.3449 - acc: 0.8623 - val_loss: 0.5000 - val_acc: 0.8091\n",
      "Epoch 46/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.3387 - acc: 0.8615 - val_loss: 0.5063 - val_acc: 0.8144\n",
      "Epoch 47/50\n",
      "10584/10584 [==============================] - 0s 21us/step - loss: 0.3370 - acc: 0.8655 - val_loss: 0.4825 - val_acc: 0.8161\n",
      "Epoch 48/50\n",
      "10584/10584 [==============================] - 0s 20us/step - loss: 0.3351 - acc: 0.8632 - val_loss: 0.5157 - val_acc: 0.8029\n",
      "Epoch 49/50\n",
      "10584/10584 [==============================] - 0s 27us/step - loss: 0.3287 - acc: 0.8680 - val_loss: 0.4814 - val_acc: 0.8197\n",
      "Epoch 50/50\n",
      "10584/10584 [==============================] - 0s 19us/step - loss: 0.3308 - acc: 0.8639 - val_loss: 0.5270 - val_acc: 0.8064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x254a93c3518>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train,\n",
    "          batch_size=100,\n",
    "          epochs=50,\n",
    "          validation_data=(X_val, y_val),\n",
    "          verbose=1,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the best model you can, experimenting with hyperparameters before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2268/2268 [==============================] - 0s 19us/step\n",
      "\n",
      " model test loss is 0.5277782116196984 accuracy is 0.8059964724528936\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y=y_test)\n",
    "print(\"\\n model test loss is \"+str(loss)+\" accuracy is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.74      0.71      0.72       312\n",
      "          2       0.69      0.60      0.64       290\n",
      "          3       0.85      0.64      0.73       331\n",
      "          4       0.89      0.95      0.92       315\n",
      "          5       0.80      0.93      0.86       322\n",
      "          6       0.71      0.87      0.78       343\n",
      "          7       0.96      0.91      0.94       355\n",
      "\n",
      "avg / total       0.81      0.81      0.80      2268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_softmax = model.predict(X_test)  # this is an n x class matrix of probabilities\n",
    "y_hat = y_softmax.argmax(axis=-1)  # this will be the class number.\n",
    "y_test_cat = y_test.argmax(axis=-1)  # our test data is also categorical\n",
    "print(classification_report(y_test_cat, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Explain in your own words what precsion, recall, and f1-score are.\n",
    "\n",
    "Precision is the Ratio of items predicted true that are actually true and not falsely true. Thus for this example, if the precision value is lower, it means that there are a lot of false positives, or classifications that are untrue. Such as something being designated a 1 when it is really a 2 for Cover_Type. Recall is the ratio of items that are true with respect to those that are true and those that are falsely negative (which should be true) Thus a low value for recall would suggest that there are for excample category 1 items that are being declared as any of the other categories. F1 is composed of the precision and recall and varies from 0 to 1 or if scaled 0 to 100. F1 is a helpful measure when uneven class distributions exist. It is often a good measure of accuracy as it takes into consideration the precision and recall.\n",
    "\n",
    "Q5: Is there a particular class that the model is best at predicting?\n",
    "It seems like class 4 and class 7 are modeled the best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Notes:\n",
    "Things I experimented with:\n",
    "Dropout All Layers, single layer, starting at 0.5, used 0.8 Didnt really improve\n",
    "More Epochs Tested to 100. Noticed that the Val_loss increased, overfitting suspected as Val_loss was about 0.2 higher than training loss. Changed it back to 50. Added layers 1 at a time up to 6. Looks like 5 layers of 54 nodes did the trick. I experimented with node sizes and when I decreased them everything seemed to go in the opposide direction. \n",
    "\n",
    "Unfortunately for me, this project had to be redone this morning and I didnt get to spend the time on it that I wanted.  A few days ago a power line near my house got struck by lightning and took out half the electronics in my house. Had to dig out an old laptop and replace my whole network despite being on surge protectors.\n",
    "\n",
    "As you can probably see above I had to specify not to use GPU as mine isnt compatible. Also I had to use a separate file to house Tensorboard. That made things easier. (Next Time I will use a separate thread)\n",
    "\n",
    "The code for that page is below:\n",
    "import os\n",
    "from tensorboard import default\n",
    "from tensorboard import program\n",
    "\n",
    "my_directory = os.getcwd()+'\\\\logs'\n",
    "tb = program.TensorBoard(default.PLUGIN_LOADERS, default.get_assets_zip_provider())\n",
    "tb.configure(argv=['--logdir', my_directory])\n",
    "tb.main()\n",
    "\n",
    "I wonder what the expected accuracy is, and if its 100% then wow... im in trouble. Please post solutions to your problems after grading if you would. (or upload to github)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
