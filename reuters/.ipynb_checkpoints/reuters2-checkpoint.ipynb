{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=None,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=None,\n",
    "                                                         test_split=0.2,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=1,\n",
    "                                                         oov_char=2,\n",
    "                                                         index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:  [1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "Training label:  3\n",
      "Length of training data 8982\n",
      "Length of test data 2246\n"
     ]
    }
   ],
   "source": [
    "#Look at a small part of the data\n",
    "print('Training data: ', x_train[0])\n",
    "print('Training label: ', y_train[0])\n",
    "print('Length of training data', len(x_train))\n",
    "print('Length of test data', len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13074"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "#Check to see what index the word sport is, a rudimentary test of the index loading\n",
    "word_index[\"sport\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word at index 13074 is:  sport\n",
      "There are 30980 words in the word index\n"
     ]
    }
   ],
   "source": [
    "#The index is organized to look up the integer value, it would be better to look up the key\n",
    "integer_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Now we can search for the word that aligns to a certain key, we looked up sport before so lets check that index\n",
    "print('The word at index 13074 is: ',integer_word_index[13074])\n",
    "\n",
    "#how many different words are in the index\n",
    "print('There are', len(integer_word_index)+1, 'words in the word index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "10000\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "#Max words in an article\n",
    "max_words = 10000\n",
    "#46 labels\n",
    "LABEL_DIMENSIONS = max(y_train)+1\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, LABEL_DIMENSIONS)\n",
    "y_test = keras.utils.to_categorical(y_test, LABEL_DIMENSIONS)\n",
    "\n",
    "print(x_train[0])\n",
    "print(len(x_train[0]))\n",
    "\n",
    "print(y_train[0])\n",
    "print(len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(vocab_size, embedding_dim, sequence_length):\n",
    "    input=Input(shape=(sequence_length,), name=\"Input\")\n",
    "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length = sequence_length,\n",
    "                          name=\"embedding\")(input)\n",
    "    lstm1 = LSTM(15, activation='tanh', return_sequences=False, dropout=0.1, recurrent_dropout=0.1, name='lstm1')(embedding)\n",
    "    output= Dense(46, activation = 'sigmoid', name='sigmoid')(lstm1)\n",
    "    model=Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(name):\n",
    "    tensorboard_callback = TensorBoard(log_dir=os.path.join(os.getcwd(), \"tensorboard_log\", name), write_graph=True, write_grads=False)\n",
    "    checkpoint_callback = ModelCheckpoint(filepath=\"./model-weights-\" + name + \".{epoch:02d}-{val_loss:.6f}.hdf5\", monitor='val_loss',\n",
    "                                          verbose=0, save_best_only=True)\n",
    "    return [tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodel = build_network(vocab_size = len(integer_word_index), embedding_dim=100, sequence_length=10000)\n",
    "callbacks =create_callbacks(\"reuters\")\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/10\n",
      "8982/8982 [==============================] - 12s 1ms/step - loss: 0.0792 - acc: 0.9607 - val_loss: 2.4635 - val_acc: 0.7533\n",
      "Epoch 2/10\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.0781 - acc: 0.9591 - val_loss: 2.4786 - val_acc: 0.7529\n",
      "Epoch 3/10\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.0922 - acc: 0.9595 - val_loss: 2.4620 - val_acc: 0.7516\n",
      "Epoch 4/10\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.0903 - acc: 0.9579 - val_loss: 2.4892 - val_acc: 0.7498\n",
      "Epoch 5/10\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.0806 - acc: 0.9575 - val_loss: 2.4595 - val_acc: 0.7498\n",
      "Epoch 6/10\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.0801 - acc: 0.9611 - val_loss: 2.4731 - val_acc: 0.7476\n",
      "Epoch 7/10\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.0773 - acc: 0.9606 - val_loss: 2.4875 - val_acc: 0.7493\n",
      "Epoch 8/10\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.0761 - acc: 0.9590 - val_loss: 2.4955 - val_acc: 0.7462\n",
      "Epoch 9/10\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.0760 - acc: 0.9606 - val_loss: 2.5130 - val_acc: 0.7467\n",
      "Epoch 10/10\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.0771 - acc: 0.9620 - val_loss: 2.5259 - val_acc: 0.7467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1947e681438>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train, y=y_train, batch_size=24, epochs=10, validation_data=(x_test, y_test),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
